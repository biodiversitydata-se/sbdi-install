---
spark:
  version: 2.4.8
  hadoop_version: 2.10.2
  working_dir: /tmp/spark/data
  master_host: live-pipelines-1
  master_port: 7077
  master_ip: live-pipelines-1
  worker_work_port: 65000
  master_ui_port: 8080
  worker_ui_port: 8085
  download_location: http://archive.apache.org/dist/spark/
  user: "spark"               # the name of the (OS)user created for spark
  user_shell: /bin/bash
  default_hosts:
    - live-pipelines-1
    - live-pipelines-2
    - live-pipelines-3
  env_extras: {}
  defaults_extras: {}
  slaves:
    - live-pipelines-2
    - live-pipelines-3
spark_installation_dir: "/data/spark"
spark_archive: "spark-{{ spark.version }}-bin-without-hadoop.tgz"
spark_download: "{{ spark.download_location }}spark-{{ spark.version }}/{{ spark_archive }}"
spark_num_workers: "{{ (groups['cluster_master']+ groups['cluster_nodes']) |length|int}}"
spark_master_ip: live-pipelines-1
ssh_key_filename: "spark_key"
