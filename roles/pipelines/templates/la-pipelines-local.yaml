general:
  hdfsSiteConfig: /data/hadoop/etc/hadoop/hdfs-site.xml
  coreSiteConfig: /data/hadoop/etc/hadoop/core-site.xml

run:
  platform: spark-cluster
  spark-cluster:
    sparkMaster: spark://live-pipelines-1:7077

fs:
  platform: hdfs
  hdfs:
    fsPath: hdfs://live-pipelines-1:9000

dwca-avro:
  inputPath: /data/dwca-export/{datasetId}/{datasetId}.zip
  tempLocation: /data/dwca-export/{datasetId}/tmp/

alaNameMatch:
  wsUrl: http://localhost:9179

sds:
  wsUrl: http://localhost:9189

collectory:
  wsUrl: https://collections.biodiversitydata.se/ws/
  httpHeaders:
    Authorization: {{ pipelines.collectory_apikey }}

imageService:
  wsUrl: https://images.biodiversitydata.se/
  httpHeaders:
    apiKey: {{ pipelines.imageservice_apikey }}

speciesListService:
  wsUrl: https://lists.biodiversitydata.se

samplingService:
  wsUrl: https://spatial.biodiversitydata.se/ws/

index:
  includeImages: false

solr:
  zkHost: live-solrcloud-1:2181,live-solrcloud-2:2181,live-solrcloud-3:2181
  includeSampling: true
  numOfPartitions: 2

geocodeConfig:
  stateProvince:
    path: {{ data_dir }}/pipelines-shp/lan
    field: LnNamn

images:
  # pipe separated list (note: YAML list not coming through correctly)
  recognisedPaths: "https://images.biodiversitydata.se|http://localhost:8082"

export:
  imageServicePath: "https://images.biodiversitydata.se/image/proxyImageThumbnailLarge?imageId={0}"

locationInfoConfig:
    stateProvinceNamesFile : {{ data_dir }}/la-pipelines/config/stateProvinces.tsv

gbifConfig:
  vocabularyConfig:
    vocabulariesPath: '{fsPath}/pipelines-vocabularies/'
