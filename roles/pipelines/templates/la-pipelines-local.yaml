general:
  hdfsSiteConfig: /data/hadoop/etc/hadoop/hdfs-site.xml
  coreSiteConfig: /data/hadoop/etc/hadoop/core-site.xml

run:
  platform: spark-cluster
  spark-cluster:
    sparkMaster: spark://master:7077

fs:
  platform: hdfs
  hdfs:
    fsPath: hdfs://master:9000

dwca-avro:
  inputPath: /data/dwca-export/{datasetId}/{datasetId}.zip
  tempLocation: /data/dwca-export/{datasetId}/tmp/

alaNameMatch:
  wsUrl: https://namematching.biodiversitydata.se/

sds:
  wsUrl: http://ala-sensitive-data-service:9189

collectory:
  wsUrl: https://collections.biodiversitydata.se/ws/
  httpHeaders:
    Authorization: {{ pipelines.collectory_apikey }}

imageService:
  #wsUrl: https://images.biodiversitydata.se/ws/
  wsUrl: http://localhost:8082/
  httpHeaders:
    apiKey: {{ pipelines.imageservice_apikey }}

speciesListService:
  wsUrl: https://lists.biodiversitydata.se

samplingService:
  wsUrl: https://spatial.biodiversitydata.se/ws/

solr:
  #zkHost: localhost:2181,localhost:2182,localhost:2183
  zkHost: solr:9983
  includeSampling: true
  numOfPartitions: 2

geocodeConfig:
  stateProvince:
    path: {{ data_dir }}/pipelines-shp/lan
    field: LnNamn

images:
  # pipe separated list (note: YAML list not coming through correctly)
  recognisedPaths: "https://images.biodiversitydata.se|http://localhost:8082"

export:
  imageServicePath: "https://images.biodiversitydata.se/image/proxyImageThumbnailLarge?imageId={0}"

locationInfoConfig:
    stateProvinceNamesFile : {{ data_dir }}/la-pipelines/config/stateProvinces.tsv

gbifConfig:
  vocabularyConfig:
    vocabulariesPath: '{fsPath}/pipelines-vocabularies/'

interpret-sh-args:
  spark-cluster:
    executor-memory: 8G #24G

uuid-sh-args:
  spark-cluster:
    executor-memory: 8G #20G

sampling-sh-args:
  spark-cluster:
    executor-memory: 8G #16G

sensitive-sh-args:
  spark-cluster:
    executor-memory: 8G #16G

index-sh-args:
  spark-cluster:
    executor-memory: 8G #20G

solr-sh-args:
  spark-cluster:
    executor-memory: 8G #22G
