- name: Install aptitude using apt
  apt: name=aptitude state=latest update_cache=yes force_apt_get=yes
  tags:
    - pipelines

- name: Install required system packages
  apt: name={{ item }} state=latest update_cache=yes
  loop: [ 'apt-transport-https', 'ca-certificates', 'curl', 'software-properties-common', 'python3-pip', 'virtualenv', 'python3-setuptools', 'unzip', 'file']
  tags:
    - pipelines

- name: Add Docker GPG apt Key
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present
  tags:
    - pipelines
    - docker

- name: Add Docker Repository
  apt_repository:
    repo: deb https://download.docker.com/linux/ubuntu bionic stable
    state: present
  tags:
    - pipelines
    - docker

- name: Update apt and install docker-ce
  apt: update_cache=yes name={{ item }} state=latest
  with_items:
    - docker-ce
    - docker-ce-cli
    - docker-compose
    - containerd.io
  tags:
    - pipelines
    - docker

- name: Create pipelines working directories
  file: path={{ item }} state=directory owner={{ spark_user }} group={{ spark_user }} recurse=true
  with_items:
    - "{{ data_dir }}/la-pipelines/config"
    - "{{ data_dir }}/spark-tmp"
    - "{{ data_dir }}/spark/conf"
    - "{{ data_dir }}/pipelines-shp"
    - "{{ data_dir }}/dwca-tmp"
    - "{{ data_dir }}/dwca-export"
    - "{{ data_dir }}/migration"
  tags:
    - pipelines
    - pipelines-config

# yq is needed by the la-pipelines script

- name: Download yq
  get_url:
    url: "https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64"
    dest: "/usr/bin/yq"
    mode: '0755'
  vars:
  tags:
    - pipelines
    - yq

# docopts is needed by the la-pipelines script

- name: Get docopts
  shell: curl -o /usr/local/bin/docopts -LJO https://github.com/docopt/docopts/releases/download/v0.6.3-rc2/docopts_linux_amd64
  when: is_arm is not defined
  tags:
    - pipelines
    - docopts

- name: Make docopts executable
  shell: chmod +x /usr/local/bin/docopts
  tags:
    - pipelines
    - docopts

- name: Update apt and install pipelines
  apt: update_cache=yes name=la-pipelines={{ pipelines_version }} force=yes
  tags:
    - pipelines
    - pipelines-apt

- name: Install pipelines config
  template:
    src: "{{ item }}"
    dest: "{{ data_dir }}/la-pipelines/config/{{ item }}"
    mode: 0644
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
    output_encoding: iso-8859-1
  with_items:
    - la-pipelines-local.yaml
  tags:
    - pipelines
    - pipelines-config

- name: Copy docker YAML files to {{ data_dir }}
  copy: src=service/{{ item }} dest={{ data_dir }}/{{ item }}
  with_items:
    - ala-sensitive-data-service.yml
  tags:
    - pipelines
    - pipelines-services
  
- name: Copy service scripts to /usr/bin
  copy: src=service/{{ item }} dest=/usr/bin/{{ item }} mode=777
  with_items:
    - ala-sensitive-data-service.sh
  tags:
    - pipelines
    - pipelines-services

- name: Copy docker service files to /etc/systemd/system
  copy: src=service/{{ item }} dest=/etc/systemd/system/{{ item }}
  with_items:
    - ala-sensitive-data-service.service
  tags:
    - pipelines
    - pipelines-services

- name: Enable services
  service: name={{ item }} enabled=yes
  with_items:
    - ala-sensitive-data-service
  tags:
    - pipelines
    - pipelines-services

- name: Start service ala-sensitive-data-service, if not running
  service:
    name: ala-sensitive-data-service
    state: started
  tags:
    - pipelines
    - pipelines-services

- name: Create HDFS base directories
  shell: "{{ hadoop_install_dir }}/bin/hdfs dfs -mkdir -p {{ item }}"
  with_items:
    - "/dwca-exports"
    - "/dwca-imports"
    - "/pipelines-data"
    - "/pipelines-outlier"
    - "/pipelines-all-datasets"
    - "/pipelines-species"
    - "/pipelines-jackknife"
    - "/pipelines-clustering"
    - "/pipelines-vocabularies"
    - "/migration"
  become: yes
  become_user: "{{ hadoop.user }}"
  tags:
    - pipelines
    - hadoop_dir

- name: Set ownership of HDFS base directories
  shell: "{{ hadoop_install_dir }}/bin/hdfs dfs -chown {{ spark_user }}:{{ spark_user }} {{ item }}"
  with_items:
    - "/dwca-exports"
    - "/dwca-imports"
    - "/pipelines-data"
    - "/pipelines-outlier"
    - "/pipelines-all-datasets"
    - "/pipelines-species"
    - "/pipelines-jackknife"
    - "/pipelines-clustering"
    - "/pipelines-vocabularies"
    - "/migration"
  become: yes
  become_user: "{{ hadoop.user }}"
  tags:
    - pipelines
    - hadoop_dir

- name: Get Gbif API vocabularies
  run_once: true
  uri:
    url: "http://api.gbif.org/v1/vocabularies/{{ item }}/releases/latest/export"
    dest: /tmp/{{ item }}.json
  with_items:
    - LifeStage
    - DegreeOfEstablishment
    - EstablishmentMeans
    - Pathway
  tags:
    - pipelines
    - pipelines-vocabularies

- name: Copy Gbif API vocabularies to dfs
  run_once: true
  shell: "{{ hadoop_install_dir }}/bin/hdfs dfs -copyFromLocal -f /tmp/{{ item }}.json /pipelines-vocabularies/ "
  with_items:
    - LifeStage
    - DegreeOfEstablishment
    - EstablishmentMeans
    - Pathway
  become_user: "{{ hadoop.user }}"
  tags:
    - pipelines
    - pipelines-vocabularies

- name: Copy states file
  copy: 
    src: "data/{{ item }}" 
    dest: "{{ data_dir }}/la-pipelines/config/{{ item }}"
    mode: 0644
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
  with_items:
    - stateProvinces.tsv
  tags:
    - pipelines
    - pipelines-states

- name: Copy and unpack SHP files (includes global layer and EEZ derived from GBIFs geocode DB)
  unarchive:
    src: "data/{{ item }}"
    dest: "{{ data_dir }}/pipelines-shp"
    owner: "{{ spark_user }}"
    group: "{{ spark_user }}"
  with_items:
    - pipelines-shp.zip
  tags:
    - pipelines
    - pipelines-layers

- name: Copy sbdi-load
  copy: src={{ item }} dest=/usr/bin/{{ item }} mode=0755
  with_items:
    - sbdi-load
  tags:
    - pipelines
    - pipelines-scripts

- name: Copy additional logging configs
  copy: src={{ item.src }} dest={{ item.dest }} owner={{ item.owner }} group={{ item.group }}
  with_items:
    - { src: "{{ logback_pipelines_conf | default('config/logback.xml') }}", dest: "{{ data_dir }}/la-pipelines/config/logback.xml", owner: "{{ spark_user }}", group: "{{ spark_user }}" }
    - { src: "{{ log4j_spark_conf | default('config/log4j-spark.properties') }}", dest: "{{ data_dir }}/spark/conf/log4j.properties", owner: "{{ spark_user }}", group: "{{ spark_user }}" }
  tags:
    - pipelines
    - pipelines-config
    